<!doctype html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>An Introduction to Normalizing Flows</title>

	<link rel="stylesheet" href="dist/reset.css">
	<link rel="stylesheet" href="dist/reveal.css">
	<link rel="stylesheet" href="dist/theme/pastel.css">
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
	<!-- Theme used for syntax highlighted code -->
	<link rel="stylesheet" href="plugin/highlight/zenburn.css">
	<link rel="stylesheet" href="plugin/chalkboard/style.css">
	<link rel="stylesheet" href="plugin/customcontrols/style.css">
</head>

<body>
	<div class="reveal">
		<div class="slides">
			<section>
				<article>
					<h2 class="r-fit-text title">Density, Flow, and Diffusion</h2>
					<hr>
					<footer style="padding-top: 2vw;">
						<x-grid columns=2 ai="center">
							<x-col class="transparent">
								<p>Kelvin Lee</p>
							</x-col>
							<x-col>
								<figure><img src="assets/intel-logo.svg" alt=""></figure>
							</x-col>
						</x-grid>
					</footer>
				</article>
			</section>
			<section>
				<section>
					<h2 class="title">Motivation</h2>
					<p>Bayesian inference focuses on posterior inference based on Bayes' theorem</p>
					<p>$p(z \vert x) = \frac{p(x \vert z) p(z)}{p(x)}$</p>
					<div class="fragment">
						<hr>
						<p>Most (interesting) real-world phenomena have intractable posteriors, likelihoods, or evidence
						</p>
						<ul>
							<li>No closed form integral: $\rho (r) = \langle \Psi \vert \hat{\rho}(r) \vert \Psi
								\rangle$
							</li>
							<li>Deep recursion: $p(\mathrm{weather} \vert \mathrm{history})$</li>
							<li>Multiple model/hypothesis testing: Bayesian evidence/Partition function $Z$</li>
						</ul>
					</div>
				</section>
				<section>
					<h2 class="title">
					</h2>
					<figure>
						<img src="assets/complexity.png" alt="">
						<figcaption>
							<p>Compromise between mechanistic understanding and
								uncertainty paralysis</p>
							<p class="fragment">Do you <em>really</em> care about the posterior, or do you just want to
								sample from it?
							</p>
						</figcaption>
					</figure>
				</section>
			</section>
			<section>
				<section>
					<h2 class="title">Variational inference</h2>
					<p>We care about the posterior&mdash;but only enough to approximate it</p>
					<p>Turn posterior estimation into an <em>optimization</em> problem, instead of a sampling one</p>
					<hr>
					<div class="fragment">
						<p>Optimize a surrogate model density $q(z)$ to approximate the true posterior $p(z
							\vert x)$ for data $x$, latent variable $z$ by minimizing the KL-divergence</p>
						<p>$q \rightarrow p$ when $\mathrm{arg~min}~\mathrm{KL}_{q(z) \vert \vert p(z \vert x)} =
							\mathbb{E}[\log q(z)] - \mathbb{E}[\log p(z,
							x)] + \log p(x)$</p>
						<hr>
					</div>
					<div class="fragment">
						<p>$\log p(x)$ is intractable but doesn't depend on $z$&mdash;so we drop it, flip the expression
							and
							call it the ELBO:</p>
						<p>$\mathrm{ELBO}(q) = \mathbb{E}[\log p(z, x)] - \mathbb{E}[\log q(z)]$</p>
					</div>
					<footer>
						<p><small><a href="https://arxiv.org/pdf/1601.00670.pdf">Blei, Kucukelbir, and McAculiffe
									(2018)</a></small></p>
					</footer>
				</section>
				<section>
					<h2 class="title">Variational autoencoder</h2>
					<figure>
						<img src="assets/vae.png" alt="">
						<figcaption>
							<p>Learn approximate $q$&mdash;factorized by <em>simple</em> distributions&mdash;by
								reconstructing data</p>
						</figcaption>
					</figure>
				</section>
				<section>
					<h2 class="title">Variational autoencoder</h2>
					<p>Approximation is limited by factorization:</p>
					<p class="fragment">
						Not many $p(x)$ is actually well-described by $q(z) \sim \mathcal{N}(\mu=0, \Sigma=1)$
					</p>
					<div class="fragment">
						<figure>
							<img src="assets/poor-fit.png" alt="">
						</figure>
						<hr>
					</div>
					<x-grid class="fragment transparent" columns="2" ai="center">
						<x-col>
							<p>Multimodal</p>
						</x-col>
						<x-col>
							<p>Not normally distributed</p>
						</x-col>
					</x-grid>
				</section>
			</section>
			<section>
				<section>
					<h2 class="title">Normalizing flows</h2>
					<x-grid columns="2" ai="top">
						<x-col>
							<p>Instead of approximating $p$, we could try model it through a series of invertible
								transformations</p>
							<p class="fragment" data-fragment-index="1">
								Create a <em>invertible</em> bijective (1:1) mapping from one random variable to
								another, along with its probability density
							</p>
							<p class="fragment" data-fragment-index="3">
								Change of variable theorem&mdash;convert densities
							</p>
						</x-col>
						<x-col class="transparent">
							<p class="fragment" data-fragment-index="2">
								$z = f(x)$ and the inverse $x = f^{-1}(z)$
							</p>
							<p class="fragment" data-fragment-index="2">
								$\int q(z)~dz = \int p(x)~dx = 1$
							</p>
							<br class="fragment" data-fragment-index="2">
							<p class="fragment" data-fragment-index="3">
								$p(x) = q(z) \left \lvert \frac{dz}{dx} \right \rvert$
							</p>
							<p class="fragment" data-fragment-index="4">
								Alternatively and more preferably: $p(x) = p(f^{-1}(z)) \left \lvert \mathrm{det}
								\frac{d f^{-1}}{dx} \right
								\rvert $
							</p>
						</x-col>
					</x-grid>
					<hr class="fragment" data-fragment-index="5">
					<p class="fragment" data-fragment-index="5">Path between $q(z)
						\leftrightarrow p(x)$ is a flow&mdash;successive transformations that preserve normalization are
						a normalizing flow</p>
				</section>
				<section>
					<h2 class="title">What makes a normalizing flow?</h2>
					<p>Main criteria is that $f$ must be invertible and differentiable; $\mathrm{det}~\mathbf{J} \neq 0$
					</p>
					<div class="fragment">
						<hr>
						<ul>
							<li>
								Linear flows: $f(x) = \mathbf{A}x + b$; $f(x) = \mathbf{PLU}x + b$
							</li>
							<li>
								Planar flows: $f(x) = x + \mathbf{u}h(\mathbf{w}^Tx + b)$
							</li>
							<li>
								Autoregressive flows: $$
							</li>
						</ul>
					</div>
					<footer><small><a href="https://arxiv.org/pdf/1505.05770.pdf">Rezende and Mohamed</a></small>
					</footer>
				</section>
				<section>
					<h2 class="title">Normalizing flows</h2>
					<figure>
						<img src="https://lilianweng.github.io/posts/2018-10-13-flow-models/normalizing-flow.png"
							alt="">
						<figcaption>
							<p><a href="https://lilianweng.github.io/posts/2018-10-13-flow-models/"
									style="color: #000">Lilian Weng's blog
									post</a></p>
						</figcaption>
					</figure>
				</section>
			</section>
			<section>
				<h2 class="title">References</h2>
				<ul>
					<li><a href="https://lilianweng.github.io/posts/2018-10-13-flow-models/">Lilian Weng's blog post on
							flow models</a></li>
					<li><a href="https://arxiv.org/pdf/1908.09257.pdf">Review by Kobyzev <em>et al.</em></a></li>
					<li><a href="https://arxiv.org/pdf/1601.00670.pdf">Variational inference: A review for
							statisticians</a></li>
					<li><a href="https://arxiv.org/pdf/1505.05770.pdf">Variational inference with Normalizing Flows</a>
					</li>
				</ul>
			</section>
		</div>
	</div>
	<div id="slide-footer" class="footer">
		<x-grid columns="3" ai="center">
			<x-col class="transparent">
				<small>Intel Accelerated Computing Systems and Graphics Group</small>
			</x-col>
			<x-col class="transparent">
				<small>High Performance Computing &middot; End User Enablement</small>
			</x-col>
			<x-col class="transparent">
				<small>Kelvin Lee</small>
			</x-col>
		</x-grid>
	</div>

	<script src="plugin/skelet/app.js"></script>
	<script src="plugin/skelet/modules.js"></script>
	<script src="dist/reveal.js"></script>
	<script src="plugin/notes/notes.js"></script>
	<script src="plugin/markdown/markdown.js"></script>
	<script src="plugin/highlight/highlight.js"></script>
	<script src="plugin/math/math.js"></script>
	<script src="plugin/zoom/zoom.js"></script>
	<script src="plugin/chalkboard/plugin.js"></script>
	<script src="plugin/customcontrols/plugin.js"></script>
	<script type="text/javascript">
		window.addEventListener("load", function () {

			revealDiv = document.querySelector("body div.reveal")
			footer = document.getElementById("slide-footer");
			revealDiv.appendChild(footer);

		});
	</script>
	<!-- <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
	<script>
		mermaid.initialize({
			startOnLoad: true,
		});
	</script> -->
	<script>
		// More info about initialization & config:
		// - https://revealjs.com/initialization/
		// - https://revealjs.com/config/
		Reveal.initialize({
			hash: true,
			slideNumber: true,
			transition: 'fade',
			transitionSpeed: 'slow',
			controlsLayout: 'edges',
			viewDistance: 50,
			preloadIframes: true,
			plugins: [RevealChalkboard, RevealMarkdown, RevealHighlight, RevealNotes, RevealMath, RevealZoom],
			chalkboard: {
				chalkEffect: 0.1,
				theme: "chalkboard"
			}
			// dependencies: [{ src: 'plugin/mermaid/mermaid.js' }]
		});
	</script>
</body>

</html>