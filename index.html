<!doctype html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>An Introduction to Normalizing Flows</title>

	<link rel="stylesheet" href="dist/reset.css">
	<link rel="stylesheet" href="dist/reveal.css">
	<link rel="stylesheet" href="dist/theme/pastel.css">
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
	<!-- Theme used for syntax highlighted code -->
	<link rel="stylesheet" href="plugin/highlight/zenburn.css">
	<link rel="stylesheet" href="plugin/chalkboard/style.css">
	<link rel="stylesheet" href="plugin/customcontrols/style.css">
</head>

<body>
	<div class="reveal">
		<div class="slides">
			<section>
				<article>
					<h2 class="r-fit-text title">Density, Flow, and Diffusion</h2>
					<hr>
					<footer style="padding-top: 2vw;">
						<x-grid columns=2 ai="center">
							<x-col class="transparent">
								<p>Kelvin Lee</p>
							</x-col>
							<x-col>
								<figure><img src="assets/intel-logo.svg" alt=""></figure>
							</x-col>
						</x-grid>
					</footer>
				</article>
			</section>
			<section>
				<section>
					<h2 class="title">Motivation</h2>
					<p>Bayesian inference focuses on posterior inference based on Bayes' theorem</p>
					<p>$p(z \vert x) = \frac{p(x \vert z) p(z)}{p(x)}$</p>
					<div class="fragment">
						<hr>
						<p>Most (interesting) real-world phenomena have intractable posteriors, likelihoods, or evidence
						</p>
						<ul>
							<li>No closed form integral: $\rho (r) = \langle \Psi \vert \hat{\rho}(r) \vert \Psi
								\rangle$
							</li>
							<li>Deep recursion: $p(\mathrm{weather} \vert \mathrm{history})$</li>
							<li>Multiple model/hypothesis testing: Bayesian evidence/Partition function $Z$</li>
						</ul>
					</div>
				</section>
				<section>
					<h2 class="title">
					</h2>
					<figure>
						<img src="assets/complexity.png" alt="">
						<figcaption>
							<p>Compromise between mechanistic understanding and
								uncertainty paralysis</p>
							<p class="fragment">Do you <em>really</em> care about the posterior, or do you just want to
								sample from it?
							</p>
						</figcaption>
					</figure>
				</section>
			</section>
			<section>
				<section>
					<h2 class="title">Variational inference</h2>
					<p>We care about the posterior&mdash;but only enough to approximate it</p>
					<p>Turn posterior estimation into an <em>optimization</em> problem, instead of a sampling one</p>
					<hr>
					<div class="fragment">
						<p>Optimize a surrogate model density $q(z)$ to approximate the true posterior $p(z
							\vert x)$ for data $x$, latent variable $z$ by minimizing the KL-divergence</p>
						<p>$q \rightarrow p$ when $\mathrm{arg~min}~\mathrm{KL}_{q(z) \vert \vert p(z \vert x)} =
							\mathbb{E}[\log q(z)] - \mathbb{E}[\log p(z,
							x)] + \log p(x)$</p>
						<hr>
					</div>
					<div class="fragment">
						<p>$\log p(x)$ is intractable but doesn't depend on $z$&mdash;so we drop it, flip the expression
							and
							call it the ELBO:</p>
						<p>$\mathrm{ELBO}(q) = \mathbb{E}[\log p(z, x)] - \mathbb{E}[\log q(z)]$</p>
					</div>
					<footer>
						<p><small><a href="https://arxiv.org/pdf/1601.00670.pdf">Blei, Kucukelbir, and McAculiffe
									(2018)</a></small></p>
					</footer>
				</section>
				<section>
					<h2 class="title">Variational autoencoder</h2>
					<figure>
						<img src="assets/vae.png" alt="">
						<figcaption>
							<p>Learn approximate $q$&mdash;factorized by <em>simple</em> distributions&mdash;by
								reconstructing data</p>
						</figcaption>
					</figure>
				</section>
				<section>
					<h2 class="title">Variational autoencoder</h2>
					<p>Approximation is limited by factorization:</p>
					<p class="fragment">
						Not many $p(x)$ is actually well-described by $q(z) \sim \mathcal{N}(\mu=0, \Sigma=1)$
					</p>
					<div class="fragment">
						<figure>
							<img src="assets/poor-fit.png" alt="">
						</figure>
						<hr>
					</div>
					<x-grid class="fragment transparent" columns="2" ai="center">
						<x-col>
							<p>Multimodal</p>
						</x-col>
						<x-col>
							<p>Not normally distributed</p>
						</x-col>
					</x-grid>
				</section>
			</section>
			<section>
				<section>
					<h2 class="title">Normalizing flows</h2>
					<x-grid columns="2" ai="top">
						<x-col>
							<p>Instead of approximating $p$, we could try model it through a series of invertible
								transformations</p>
							<p class="fragment" data-fragment-index="1">
								Create a <em>invertible</em> bijective (1:1) mapping from one random variable to
								another, along with its probability density
							</p>
							<p class="fragment" data-fragment-index="3">
								Change of variable theorem&mdash;convert densities
							</p>
						</x-col>
						<x-col class="transparent">
							<p class="fragment" data-fragment-index="2">
								$z = f(x)$ and the inverse $x = f^{-1}(z)$
							</p>
							<p class="fragment" data-fragment-index="2">
								$\int q(z)~dz = \int p(x)~dx = 1$
							</p>
							<br class="fragment" data-fragment-index="2">
							<p class="fragment" data-fragment-index="3">
								$p(x) = q(z) \left \lvert \frac{dz}{dx} \right \rvert$
							</p>
							<p class="fragment" data-fragment-index="4">
								Alternatively and more preferably: $p(x) = p(f^{-1}(z)) \left \lvert \mathrm{det}
								\frac{d f^{-1}}{dx} \right
								\rvert $
							</p>
						</x-col>
					</x-grid>
					<hr class="fragment" data-fragment-index="5">
					<p class="fragment" data-fragment-index="5">Path between $q(z)
						\leftrightarrow p(x)$ is a flow&mdash;successive transformations that preserve normalization are
						a normalizing flow</p>
				</section>
				<section>
					<h2 class="title">What makes a normalizing flow?</h2>
					<p>Main criteria is that $f$ must be invertible and differentiable; $\mathrm{det}~\mathbf{J} \neq 0$
					</p>
					<div class="fragment">
						<hr>
						<ul>
							<li>
								Linear flows: $f(x) = \mathbf{A}x + b$; $f(x) = \mathbf{PLU}x + b$
							</li>
							<li>
								Planar flows: $f(x) = x + \mathbf{u}h(\mathbf{w}^Tx + b)$
							</li>
							<li>
								Autoregressive flows: $f(x;t) = \tau(c(x;t-1), x; t)$
							</li>
						</ul>
					</div>
					<footer><small><a href="https://arxiv.org/pdf/1505.05770.pdf">Rezende and Mohamed</a></small>
					</footer>
				</section>
				<section>
					<h2 class="title">Normalizing flows</h2>
					<figure>
						<img src="https://lilianweng.github.io/posts/2018-10-13-flow-models/normalizing-flow.png"
							alt="">
						<figcaption>
							<p><a href="https://lilianweng.github.io/posts/2018-10-13-flow-models/"
									style="color: #000">Lilian Weng's blog
									post</a></p>
						</figcaption>
					</figure>
				</section>
				<section>
					<figure>
						<img src="assets/normalizing-flow.png" alt="" style="max-height: 500px">
						<figcaption>
							<p>Transform initial diagonal Gaussian into discontinuous, multimodal density</p>
							<p><a href="https://proceedings.mlr.press/v37/rezende15.pdf" style="color: #000">Rezende &
									Mohamed, 2015</a></p>
						</figcaption>
					</figure>
				</section>
				<section>
					<h2 class="title">Continuous normalizing flows</h2>
					<x-grid columns="3" ai="center">
						<x-col>
							<p>Previous definition uses discrete neural network layers to chain a sequence of
								transformations
							</p>
							<p>Reframe as a neural network with continuous depth&mdash;a differential equation</p>
							<p>Example is <span style="font-variant: small-caps;">FFJORD</span>, which includes linear
								time log density estimation</p>
						</x-col>
						<x-col span="2+2">
							<figure><img src="assets/ffjord.png" alt="" style="max-height: 400px">
								<figcaption>
									<p><a style="color: #000" href="https://arxiv.org/pdf/1810.01367.pdf">Grathwohl,
											Chen, Bettencourt,
											Sutskever, and Duvenaud (2018)</a></p>
								</figcaption>
							</figure>
						</x-col>
					</x-grid>
				</section>
				<section>
					<h2 class="title">Optimal transport</h2>
					<x-grid columns="4" ai="center">
						<x-col span="1+2">
							<p>Normalizing flows typically conditioned by maximizing log likelihood via minimizing
								KL-divergence
							</p>
							<br>
							<p>Reduce time complexity in the integrator <em>and</em> add expressiveness by reformulating
								within an optimal transport and control theory framework</p>
							<br>
							<p class="fragment" data-fragment-index="1">
								Add an $L_2$ transport cost term to log likelihood: $L(x,T) = \int^T_0 \frac{1}{2}
								\| \mathbf{v}(z(x,t), t) \|^2~dt $
							</p>
							<p class="fragment" data-fragment-index="1">
								$\mathbf{v}$ expressed as $-\nabla \Phi$, and satifies Hamilton-Jacobi-Bellman
								equations. $\Phi$ parameterized as a ResNet.
							</p>
						</x-col>
						<x-col span="3+2">
							<figure>
								<img style="max-height: 400px" src="assets/ot-flow.png" alt="">
								<figcaption>
									<p><a style="color: #000" href="https://arxiv.org/pdf/2006.00104.pdf">Onken, Fung,
											Li, and Ruthotto
											(2021)</a></p>
								</figcaption>
							</figure>
						</x-col>
					</x-grid>
				</section>
			</section>
			<section>
				<section>
					<h2 class="title">Diffusion models</h2>
					<p>Flow models are restricted by form of $f$&mdash;requirements on invertibility and
						$\mathrm{det}~\mathbf{J}$</p>
					<div class="fragment">
						<hr>
						<p>Alternatively, frame density estimation as a difussion process</p>
						<p>
							For $x_0 \sim p(x)$, forward diffusion means adding Gaussian noise over $T$ timesteps
						</p>
						<p class="fragment" data-fragment-index="2">
							$p(x_t \vert x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t}x_{t-1},\beta_t \mathbf{I})$,
							$\{\beta \in (0, 1)\}^T_{t=1}$
						</p>
						<br class="fragment" data-fragment-index="2">
						<p class="fragment" data-fragment-index="3">
							$p(x_{1:T} \vert x_0) = \prod^T_{t=1} p(x_t \vert x_{t-1})$
						</p>
						<p class="fragment" data-fragment-index="3">
							$T \rightarrow \infty, x_T \sim \mathcal{N}(\mu, \sigma^2 \mathbf{I})$
						</p>
					</div>
					<footer><small><a href="https://www.stats.ox.ac.uk/~teh/research/compstats/WelTeh2011a.pdf">Welling
								and Teh, (2011)</a></small></footer>
				</section>
				<section>
					<figure>
						<img src="assets/diffusion-dag.png" alt="">
						<figcaption>
							<p><a href="https://arxiv.org/pdf/2006.11239.pdf">Ho, Jain, and Abbeel (2020)</a></p>
						</figcaption>
					</figure>
				</section>
			</section>
			<section>
				<h2 class="title">References</h2>
				<ul>
					<li><a href="https://lilianweng.github.io/posts/2018-10-13-flow-models/">Lilian Weng's blog post on
							flow models</a></li>
					<li><a href="https://arxiv.org/pdf/1908.09257.pdf">Review by Kobyzev <em>et al.</em></a></li>
					<li><a href="https://arxiv.org/pdf/1601.00670.pdf">Variational inference: A review for
							statisticians</a></li>
					<li><a href="https://arxiv.org/pdf/1505.05770.pdf">Variational inference with Normalizing Flows</a>
					</li>
				</ul>
			</section>
		</div>
	</div>
	<div id="slide-footer" class="footer">
		<x-grid columns="3" ai="center">
			<x-col class="transparent">
				<small>Intel Accelerated Computing Systems and Graphics Group</small>
			</x-col>
			<x-col class="transparent">
				<small>High Performance Computing &middot; End User Enablement</small>
			</x-col>
			<x-col class="transparent">
				<small>Kelvin Lee</small>
			</x-col>
		</x-grid>
	</div>

	<script src="plugin/skelet/app.js"></script>
	<script src="plugin/skelet/modules.js"></script>
	<script src="dist/reveal.js"></script>
	<script src="plugin/notes/notes.js"></script>
	<script src="plugin/markdown/markdown.js"></script>
	<script src="plugin/highlight/highlight.js"></script>
	<script src="plugin/math/math.js"></script>
	<script src="plugin/zoom/zoom.js"></script>
	<script src="plugin/chalkboard/plugin.js"></script>
	<script src="plugin/customcontrols/plugin.js"></script>
	<script type="text/javascript">
		window.addEventListener("load", function () {

			revealDiv = document.querySelector("body div.reveal")
			footer = document.getElementById("slide-footer");
			revealDiv.appendChild(footer);

		});
	</script>
	<!-- <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
	<script>
		mermaid.initialize({
			startOnLoad: true,
		});
	</script> -->
	<script>
		// More info about initialization & config:
		// - https://revealjs.com/initialization/
		// - https://revealjs.com/config/
		Reveal.initialize({
			hash: true,
			slideNumber: true,
			transition: 'fade',
			transitionSpeed: 'slow',
			controlsLayout: 'edges',
			viewDistance: 50,
			preloadIframes: true,
			plugins: [RevealChalkboard, RevealMarkdown, RevealHighlight, RevealNotes, RevealMath, RevealZoom],
			chalkboard: {
				chalkEffect: 0.1,
				theme: "chalkboard"
			}
			// dependencies: [{ src: 'plugin/mermaid/mermaid.js' }]
		});
	</script>
</body>

</html>